{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_rag.chat.chat_openai import ChatOpenAI\n",
    "from simple_rag.prompt.template import visual_language_generation\n",
    "from simple_rag.utils import connect_to_database, insert_to_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Connection to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Successfully Get Access Database\n"
     ]
    }
   ],
   "source": [
    "collection = connect_to_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Large Language Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 1: System's prompt take a short instruct from user's prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.request(system_prompt=visual_language_generation, user_prompt=\"British shorthair\")\n",
    "insert_to_db(collection=collection, data=response)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 2: System's prompt take a long instruct from user's prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¾ Warning: Failed to refine the output of LLM because: Extra data: line 1 column 2 (char 1)\n",
      "ðŸ”¥ Successfully Log Request to Database\n"
     ]
    }
   ],
   "source": [
    "structured_sample = {\n",
    "    \"id\": \"user_oriented_task_0\",\n",
    "    \"motivation_app\": \"Grammarly\",\n",
    "    \"instruction\": \"The sentence you are given might be too wordy, complicated, or unclear. Rewrite the sentence and make your writing clearer by keeping it concise. Whenever possible, break complex sentences into multiple sentences and eliminate unnecessary words.\",\n",
    "    \"instances\": [\n",
    "        {\n",
    "            \"input\": \"If you have any questions about my rate or if you find it necessary to increase or decrease the scope for this project, please let me know.\",\n",
    "            \"output\": \"If you have any questions about my rate or find it necessary to increase or decrease this project's scope, please let me know.\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "def gen_prompt_no_input(ins, outp):\n",
    "\n",
    "    sys_prompt = \"You are a helpful, precise but picky assistant for checking the quality of the answer to a given instruction.\"\n",
    "    prompt_template = \"[Instruction]\\n{ins}\\n\\n[The Start of Answer]\\n{outp}\\n\\n[The End of Answer]\\n\\n[System]\\n{criteria}\\n\\n\"\n",
    "    criteria = (\n",
    "        \"We would like you to answer several questions related to the quality of the answer to the given instruction. \\n\"\n",
    "        + \"1. Why this answer is not good for the given instruction? Analyse based on the Helpfulness, Relevance, Accuracy, Level of Details, and Structure. \\n\"\n",
    "        + \"2. Based on the reason you provided, please generate a better answer while preserving the same content. To achieve that, you may want to adjust the level of details, add bullet points, or use comprehensive words, etc. The answer should be in the format of [Better Answer] your answer [End] \\n\"\n",
    "    )\n",
    "    prompt = prompt_template.format(ins=ins, outp=outp, criteria=criteria)\n",
    "    return sys_prompt, prompt\n",
    "system_prompt, user_prompt = gen_prompt_no_input(ins=structured_sample[\"instruction\"], outp=structured_sample[\"instances\"][0][\"output\"])\n",
    "response = llm.request(system_prompt=system_prompt, user_prompt=user_prompt, max_tokens=2048)\n",
    "insert_to_db(collection=collection, data=response)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
